{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get similarity scores for a list of cases\n",
    "\n",
    "Alternative for get_similarities() from similiarity_matrix.py\n",
    "\n",
    "Uses closure for workhorse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_get_similarity(similarity_matrix, CLid_to_index):\n",
    "    \"\"\"\n",
    "    helper funciton for get_similarities\n",
    "    \"\"\"\n",
    "    def f(CLid_pair):\n",
    "        try:\n",
    "            ida = CLid_to_index[CLid_pair[0]]\n",
    "            idb = CLid_to_index[CLid_pair[1]]\n",
    "        \n",
    "            return similarity_matrix[ida, idb]\n",
    "        except KeyError:\n",
    "            return np.nan\n",
    "        \n",
    "    return f\n",
    "    \n",
    "def get_similarities(similarity_matrix, CLid_pair, CLid_to_index):\n",
    "    \"\"\"\n",
    "    Returns the similarities for cases index by CL ids as a list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    similarity_matrix: precomputed similarity matrix\n",
    "\n",
    "    CLid_A, CLid_B: two lists of CL ids whose similarities we want\n",
    "\n",
    "    CLid_to_index: dict that maps CL ids to similarity_matrix indices\n",
    "    \"\"\"\n",
    "    get_similarity = make_get_similarity(similarity_matrix, CLid_to_index)\n",
    "    return [get_similarity( pair) for pair in CLid_pair]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample absent edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this ended up being a lot slower than sampling an arbitrary pair of vertices then checking the conditions\n",
    "def sample_absent_edges(G, num_samples, active_years, seed=None):\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    # citing cases must be in active_years\n",
    "    possible_ing_cases = set(G.vs.select(year_ge = min(active_years), year_le = max(active_years)))\n",
    "\n",
    "    samples = set()\n",
    "    present_edges = set(G.get_edgelist()) \n",
    "    \n",
    "    while len(samples) < num_samples:\n",
    "        # sample a citing case\n",
    "        ing_vertex = random.sample(possible_ing_cases, 1)[0]\n",
    "        \n",
    "        # get possible cited cases\n",
    "        # cited cases must be strictly before citing case\n",
    "        possible_ed_cases = G.vs.select(year_le = ing_vertex['year'] - 1)\n",
    "        \n",
    "        # sample a cited case\n",
    "        ed_vertex = random.sample(possible_ed_cases, 1)[0]\n",
    "        \n",
    "        absent_edge = (ing_vertex.index, ed_vertex.index)\n",
    "        \n",
    "        if (absent_edge not in present_edges) and (absent_edge not in samples):\n",
    "            samples.add(absent_edge)\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textfile chunk iterator\n",
    "\n",
    "this could be helpful\n",
    "http://stackoverflow.com/questions/31784011/scikit-learn-fitting-data-into-chunks-vs-fitting-it-all-at-once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class textfile_chunks:\n",
    "    def __init__(self, paths, chunk_size):\n",
    "        self.i = 0\n",
    "\n",
    "        self.paths = paths\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_files = len(paths)\n",
    "\n",
    "        self.num_chunks = ceil(float(self.num_files)/self.chunk_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        if self.i < self.num_chunks:\n",
    "\n",
    "            # file paths to return\n",
    "            file_paths = self.paths[self.i:min(self.i + self.chunk_size,\n",
    "                                    self.num_files)]\n",
    "\n",
    "            # read in files and put them into dict\n",
    "            files = {}\n",
    "            for path in file_paths:\n",
    "                text = open(path, 'r').read()\n",
    "                files[path] = text_normalization(text)\n",
    "\n",
    "            self.i += 1\n",
    "\n",
    "            return files\n",
    "\n",
    "        else:\n",
    "            raise StopIteration()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
