{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M1: direction ~ Lag1 + Lag2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We want to predict if the stock market will go up or down based on the returns from the previous couple days trading history.\n",
    "We will fit two models then compare them on a test set.\n",
    "\n",
    "create a training set and a test set\n",
    "\n",
    "using logistic regression fit the following two models on the training set\n",
    "\n",
    "M1: direction ~ Lag1 + Lag2\n",
    "M2: direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume\n",
    "\n",
    "output the beta coefficients for M1 and M2 (for M2 they should be close to those from ISLR)\n",
    "\n",
    "evaluate M1 and M2 on the training and test set using the following three loss functions\n",
    "L1: 0-1 loss (this is just the percentage of correct predictions)\n",
    "L2: cross entropy loss\n",
    "L3: logistic loss\n",
    "e.g. for each M1, M2 you should have a 3x2 table of [train, test] x [L1, L2, L3]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.linear_model as skl_lm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, log_loss\n",
    "from sklearn import preprocessing\n",
    "from sklearn import neighbors\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset from R into desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninstall.packages(\"ISLR\")\\nlibrary(ISLR)\\ninstall.packages(\"xlsx\")\\nlibrary(xlsx)\\n\\nwrite.xlsx(Smarket, \"c:/Users/Michael/Desktop/Smarket.xlsx\")\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "install.packages(\"ISLR\")\n",
    "library(ISLR)\n",
    "install.packages(\"xlsx\")\n",
    "library(xlsx)\n",
    "\n",
    "write.xlsx(Smarket, \"c:/Users/Michael/Desktop/Smarket.xlsx\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume  Today Direction\n",
       "1     2001  0.381 -0.192 -2.624 -1.055  5.010  1.19130  0.959        Up\n",
       "2     2001  0.959  0.381 -0.192 -2.624 -1.055  1.29650  1.032        Up\n",
       "3     2001  1.032  0.959  0.381 -0.192 -2.624  1.41120 -0.623      Down\n",
       "4     2001 -0.623  1.032  0.959  0.381 -0.192  1.27600  0.614        Up\n",
       "5     2001  0.614 -0.623  1.032  0.959  0.381  1.20570  0.213        Up\n",
       "6     2001  0.213  0.614 -0.623  1.032  0.959  1.34910  1.392        Up\n",
       "7     2001  1.392  0.213  0.614 -0.623  1.032  1.44500 -0.403      Down\n",
       "8     2001 -0.403  1.392  0.213  0.614 -0.623  1.40780  0.027        Up\n",
       "9     2001  0.027 -0.403  1.392  0.213  0.614  1.16400  1.303        Up\n",
       "10    2001  1.303  0.027 -0.403  1.392  0.213  1.23260  0.287        Up\n",
       "11    2001  0.287  1.303  0.027 -0.403  1.392  1.30900 -0.498      Down\n",
       "12    2001 -0.498  0.287  1.303  0.027 -0.403  1.25800 -0.189      Down\n",
       "13    2001 -0.189 -0.498  0.287  1.303  0.027  1.09800  0.680        Up\n",
       "14    2001  0.680 -0.189 -0.498  0.287  1.303  1.05310  0.701        Up\n",
       "15    2001  0.701  0.680 -0.189 -0.498  0.287  1.14980 -0.562      Down\n",
       "16    2001 -0.562  0.701  0.680 -0.189 -0.498  1.29530  0.546        Up\n",
       "17    2001  0.546 -0.562  0.701  0.680 -0.189  1.11880 -1.747      Down\n",
       "18    2001 -1.747  0.546 -0.562  0.701  0.680  1.04840  0.359        Up\n",
       "19    2001  0.359 -1.747  0.546 -0.562  0.701  1.01300 -0.151      Down\n",
       "20    2001 -0.151  0.359 -1.747  0.546 -0.562  1.05960 -0.841      Down\n",
       "21    2001 -0.841 -0.151  0.359 -1.747  0.546  1.15830 -0.623      Down\n",
       "22    2001 -0.623 -0.841 -0.151  0.359 -1.747  1.10720 -1.334      Down\n",
       "23    2001 -1.334 -0.623 -0.841 -0.151  0.359  1.07550  1.183        Up\n",
       "24    2001  1.183 -1.334 -0.623 -0.841 -0.151  1.03910 -0.865      Down\n",
       "25    2001 -0.865  1.183 -1.334 -0.623 -0.841  1.07520 -0.218      Down\n",
       "26    2001 -0.218 -0.865  1.183 -1.334 -0.623  1.15030  0.812        Up\n",
       "27    2001  0.812 -0.218 -0.865  1.183 -1.334  1.15370 -1.891      Down\n",
       "28    2001 -1.891  0.812 -0.218 -0.865  1.183  1.25720 -1.736      Down\n",
       "29    2001 -1.736 -1.891  0.812 -0.218 -0.865  1.11220 -1.851      Down\n",
       "30    2001 -1.851 -1.736 -1.891  0.812 -0.218  1.20850 -0.195      Down\n",
       "...    ...    ...    ...    ...    ...    ...      ...    ...       ...\n",
       "1221  2005  0.179 -0.385 -0.078  0.305  0.845  2.12158  0.941        Up\n",
       "1222  2005  0.941  0.179 -0.385 -0.078  0.305  2.29804  0.440        Up\n",
       "1223  2005  0.440  0.941  0.179 -0.385 -0.078  2.45329  0.527        Up\n",
       "1224  2005  0.527  0.440  0.941  0.179 -0.385  2.11735  0.508        Up\n",
       "1225  2005  0.508  0.527  0.440  0.941  0.179  2.29142  0.347        Up\n",
       "1226  2005  0.347  0.508  0.527  0.440  0.941  1.98540  0.209        Up\n",
       "1227  2005  0.209  0.347  0.508  0.527  0.440  0.72494 -0.851      Down\n",
       "1228  2005 -0.851  0.209  0.347  0.508  0.527  2.01690  0.002        Up\n",
       "1229  2005  0.002 -0.851  0.209  0.347  0.508  2.26834 -0.636      Down\n",
       "1230  2005 -0.636  0.002 -0.851  0.209  0.347  2.37469  1.216        Up\n",
       "1231  2005  1.216 -0.636  0.002 -0.851  0.209  2.61483  0.032        Up\n",
       "1232  2005  0.032  1.216 -0.636  0.002 -0.851  2.12558 -0.236      Down\n",
       "1233  2005 -0.236  0.032  1.216 -0.636  0.002  2.32584  0.128        Up\n",
       "1234  2005  0.128 -0.236  0.032  1.216 -0.636  2.11074 -0.501      Down\n",
       "1235  2005 -0.501  0.128 -0.236  0.032  1.216  2.09383 -0.122      Down\n",
       "1236  2005 -0.122 -0.501  0.128 -0.236  0.032  2.17830  0.281        Up\n",
       "1237  2005  0.281 -0.122 -0.501  0.128 -0.236  1.89629  0.084        Up\n",
       "1238  2005  0.084  0.281 -0.122 -0.501  0.128  1.87655  0.555        Up\n",
       "1239  2005  0.555  0.084  0.281 -0.122 -0.501  2.39002  0.419        Up\n",
       "1240  2005  0.419  0.555  0.084  0.281 -0.122  2.14552 -0.141      Down\n",
       "1241  2005 -0.141  0.419  0.555  0.084  0.281  2.18059 -0.285      Down\n",
       "1242  2005 -0.285 -0.141  0.419  0.555  0.084  2.58419 -0.584      Down\n",
       "1243  2005 -0.584 -0.285 -0.141  0.419  0.555  2.20881 -0.024      Down\n",
       "1244  2005 -0.024 -0.584 -0.285 -0.141  0.419  1.99669  0.252        Up\n",
       "1245  2005  0.252 -0.024 -0.584 -0.285 -0.141  2.06517  0.422        Up\n",
       "1246  2005  0.422  0.252 -0.024 -0.584 -0.285  1.88850  0.043        Up\n",
       "1247  2005  0.043  0.422  0.252 -0.024 -0.584  1.28581 -0.955      Down\n",
       "1248  2005 -0.955  0.043  0.422  0.252 -0.024  1.54047  0.130        Up\n",
       "1249  2005  0.130 -0.955  0.043  0.422  0.252  1.42236 -0.298      Down\n",
       "1250  2005 -0.298  0.130 -0.955  0.043  0.422  1.38254 -0.489      Down\n",
       "\n",
       "[1250 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"c:/Users/Michael/Desktop/Smarket.xlsx\")\n",
    "df\n",
    "#df[(df['Year'] != 2005)][['Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5', 'Volume']]\n",
    "#df[(df['Year'] != 2005)]['Direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year   Lag1   Lag2   Lag3   Lag4   Lag5   Volume  Today Direction\n",
      "1    2001  0.381 -0.192 -2.624 -1.055  5.010  1.19130  0.959        Up\n",
      "2    2001  0.959  0.381 -0.192 -2.624 -1.055  1.29650  1.032        Up\n",
      "3    2001  1.032  0.959  0.381 -0.192 -2.624  1.41120 -0.623      Down\n",
      "4    2001 -0.623  1.032  0.959  0.381 -0.192  1.27600  0.614        Up\n",
      "5    2001  0.614 -0.623  1.032  0.959  0.381  1.20570  0.213        Up\n",
      "6    2001  0.213  0.614 -0.623  1.032  0.959  1.34910  1.392        Up\n",
      "7    2001  1.392  0.213  0.614 -0.623  1.032  1.44500 -0.403      Down\n",
      "8    2001 -0.403  1.392  0.213  0.614 -0.623  1.40780  0.027        Up\n",
      "9    2001  0.027 -0.403  1.392  0.213  0.614  1.16400  1.303        Up\n",
      "10   2001  1.303  0.027 -0.403  1.392  0.213  1.23260  0.287        Up\n",
      "11   2001  0.287  1.303  0.027 -0.403  1.392  1.30900 -0.498      Down\n",
      "12   2001 -0.498  0.287  1.303  0.027 -0.403  1.25800 -0.189      Down\n",
      "13   2001 -0.189 -0.498  0.287  1.303  0.027  1.09800  0.680        Up\n",
      "14   2001  0.680 -0.189 -0.498  0.287  1.303  1.05310  0.701        Up\n",
      "15   2001  0.701  0.680 -0.189 -0.498  0.287  1.14980 -0.562      Down\n",
      "16   2001 -0.562  0.701  0.680 -0.189 -0.498  1.29530  0.546        Up\n",
      "17   2001  0.546 -0.562  0.701  0.680 -0.189  1.11880 -1.747      Down\n",
      "18   2001 -1.747  0.546 -0.562  0.701  0.680  1.04840  0.359        Up\n",
      "19   2001  0.359 -1.747  0.546 -0.562  0.701  1.01300 -0.151      Down\n",
      "20   2001 -0.151  0.359 -1.747  0.546 -0.562  1.05960 -0.841      Down\n",
      "21   2001 -0.841 -0.151  0.359 -1.747  0.546  1.15830 -0.623      Down\n",
      "22   2001 -0.623 -0.841 -0.151  0.359 -1.747  1.10720 -1.334      Down\n",
      "23   2001 -1.334 -0.623 -0.841 -0.151  0.359  1.07550  1.183        Up\n",
      "24   2001  1.183 -1.334 -0.623 -0.841 -0.151  1.03910 -0.865      Down\n",
      "25   2001 -0.865  1.183 -1.334 -0.623 -0.841  1.07520 -0.218      Down\n",
      "26   2001 -0.218 -0.865  1.183 -1.334 -0.623  1.15030  0.812        Up\n",
      "27   2001  0.812 -0.218 -0.865  1.183 -1.334  1.15370 -1.891      Down\n",
      "28   2001 -1.891  0.812 -0.218 -0.865  1.183  1.25720 -1.736      Down\n",
      "29   2001 -1.736 -1.891  0.812 -0.218 -0.865  1.11220 -1.851      Down\n",
      "30   2001 -1.851 -1.736 -1.891  0.812 -0.218  1.20850 -0.195      Down\n",
      "..    ...    ...    ...    ...    ...    ...      ...    ...       ...\n",
      "213  2001  0.090  0.186  1.856 -0.177  0.158  1.45450 -0.314      Down\n",
      "214  2001 -0.314  0.090  0.186  1.856 -0.177  1.33740  1.090        Up\n",
      "215  2001  1.090 -0.314  0.090  0.186  1.856  1.31680 -0.730      Down\n",
      "216  2001 -0.730  1.090 -0.314  0.090  0.186  1.33020 -0.493      Down\n",
      "217  2001 -0.493 -0.730  1.090 -0.314  0.090  1.02930  1.171        Up\n",
      "218  2001  1.171 -0.493 -0.730  1.090 -0.314  0.41030  0.615        Up\n",
      "219  2001  0.615  1.171 -0.493 -0.730  1.090  1.12980 -0.684      Down\n",
      "220  2001 -0.684  0.615  1.171 -0.493 -0.730  1.28800 -1.825      Down\n",
      "221  2001 -1.825 -0.684  0.615  1.171 -0.493  1.42370  1.035        Up\n",
      "222  2001  1.035 -1.825 -0.684  0.615  1.171  1.37570 -0.066      Down\n",
      "223  2001 -0.066  1.035 -1.825 -0.684  0.615  1.34360 -0.838      Down\n",
      "224  2001 -0.838 -0.066  1.035 -1.825 -0.684  1.20290  1.319        Up\n",
      "225  2001  1.319 -0.838 -0.066  1.035 -1.825  1.31850  2.232        Up\n",
      "226  2001  2.232  1.319 -0.838 -0.066  1.035  1.76530 -0.278      Down\n",
      "227  2001 -0.278  2.232  1.319 -0.838 -0.066  1.48790 -0.753      Down\n",
      "228  2001 -0.753 -0.278  2.232  1.319 -0.838  1.24820 -1.587      Down\n",
      "229  2001 -1.587 -0.753 -0.278  2.232  1.319  1.21870 -0.278      Down\n",
      "230  2001 -0.278 -1.587 -0.753 -0.278  2.232  1.36720  0.027        Up\n",
      "231  2001  0.027 -0.278 -1.587 -0.753 -0.278  1.44970 -1.556      Down\n",
      "232  2001 -1.556  0.027 -0.278 -1.587 -0.753  1.51150  0.331        Up\n",
      "233  2001  0.331 -1.556  0.027 -0.278 -1.587  1.30680  1.003        Up\n",
      "234  2001  1.003  0.331 -1.556  0.027 -0.278  1.26040  0.755        Up\n",
      "235  2001  0.755  1.003  0.331 -1.556  0.027  1.35400  0.581        Up\n",
      "236  2001  0.581  0.755  1.003  0.331 -1.556  1.48490 -0.838      Down\n",
      "237  2001 -0.838  0.581  0.755  1.003  0.331  1.49050  0.435        Up\n",
      "238  2001  0.435 -0.838  0.581  0.755  1.003  1.69400 -0.021      Down\n",
      "239  2001 -0.021  0.435 -0.838  0.581  0.755  0.43967  0.412        Up\n",
      "240  2001  0.412 -0.021  0.435 -0.838  0.581  0.79110  0.675        Up\n",
      "241  2001  0.675  0.412 -0.021  0.435 -0.838  0.87630  0.336        Up\n",
      "242  2001  0.336  0.675  0.412 -0.021  0.435  0.91740 -1.115      Down\n",
      "\n",
      "[242 rows x 9 columns]\n",
      "(2001, 0.38100000000000001, -0.192, -2.6240000000000001, -1.0549999999999999, 5.0099999999999998, 1.1913, 0.95899999999999996, u'Up')\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "row = df.loc[df['Year'] == 2001]\n",
    "print row\n",
    "print tuple(row.iloc[0].tolist())\n",
    "\n",
    "print type(row.iloc[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001L, 0.381, -0.192, -2.624, -1.055, 5.01, 1.1913, 0.959, u'Up')\n"
     ]
    }
   ],
   "source": [
    "df.loc[df['Year'] == 2001].values.tolist()\n",
    "\n",
    "tuple_of_tuples = tuple(tuple(x) for x in df.values.tolist())[0]\n",
    "print tuple_of_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on Training Set = All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  [u'Down' u'Up']\n",
      "coefficients:  [[-0.07132777 -0.04437961]]\n",
      "intercept : [ 0.07424785]\n"
     ]
    }
   ],
   "source": [
    "y_train = df['Direction']\n",
    "\n",
    "# dataset pre-2005--for training set\n",
    "x_train = df[['Lag1', 'Lag2']]\n",
    "\n",
    "clf = skl_lm.LogisticRegression(solver='newton-cg')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print 'classes: ',clf.classes_\n",
    "print 'coefficients: ',clf.coef_\n",
    "print 'intercept :', clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire Probabilities of Direction == Yes for Training Set = All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 252 X 2 Matrix, where column = probability for DOWN direction, probability for UP direction--reference: clf.classes_\n",
    "prob = clf.predict_proba(x_train)\n",
    "\n",
    "# predicted probabilities for ALL years for UP direction\n",
    "prob_up = prob[:,1:2]\n",
    "\n",
    "# convert to list\n",
    "prob_up2 = [i.tolist()[0] for i in prob_up]\n",
    "\n",
    "y_predicted = []\n",
    "for i in prob_up2:\n",
    "    if i>0.5:\n",
    "        y_predicted.append(\"Up\")\n",
    "    else:\n",
    "        y_predicted.append(\"Down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1: 0-1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 (0-1 loss):  0.472\n"
     ]
    }
   ],
   "source": [
    "right_prediction = [i for i,j in zip(y_train, y_predicted) if i==j]\n",
    "number_right = len(right_prediction)\n",
    "zero_one_loss = number_right/len(y_predicted)\n",
    "\n",
    "# I had to get the percentage wrong; not the percentage correct (same as error rate in classification)\n",
    "print \"L1 (0-1 loss): \", 1-zero_one_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 (Cross Entropy Loss) by sci-kit:  0.69136137137\n",
      "L2 (Cross Entropy Loss) by manually:  0.69136137137\n"
     ]
    }
   ],
   "source": [
    "# source: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "#         https://en.wikipedia.org/wiki/Cross_entropy\n",
    "\n",
    "# using sci-kit:\n",
    "y_train2 = []\n",
    "for i in y_train:\n",
    "    if i=='Up':\n",
    "        y_train2.append(1)\n",
    "    else:\n",
    "        y_train2.append(0)\n",
    "\n",
    "print \"L2 (Cross Entropy Loss) by sci-kit: \", log_loss(y_train2, prob_up2)\n",
    "\n",
    "# manually:\n",
    "def ln(x):\n",
    "    return np.log(x)\n",
    "\n",
    "cross_entropy_losses = [-i*ln(j)-(1-i)*ln(1-j) for i,j in zip(y_train2, prob_up2)]\n",
    "print \"L2 (Cross Entropy Loss) by manually: \", sum(cross_entropy_losses)/len(cross_entropy_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3: Logistic Loss with ln(2) vs. without ln(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3 (Logistic Loss) with ln(2):  0.830742496587\n",
      "L3 (Logistic Loss) without ln(2):  0.575826819281\n"
     ]
    }
   ],
   "source": [
    "# source (with ln(2)): https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
    "\n",
    "# source (without ln(2)): https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf\n",
    "#                         https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions\n",
    "#                         http://www.cs.cmu.edu/~yandongl/loss.html\n",
    "\n",
    "logistic_losses = [(1/ln(2))*(ln(1+np.exp(-i*j))) for i,j in zip(y_train2, prob_up2)]        \n",
    "print \"L3 (Logistic Loss) with ln(2): \", sum(logistic_losses)/len(logistic_losses)\n",
    "\n",
    "logistic_losses = [ln(1+np.exp(-i*j)) for i,j in zip(y_train2, prob_up2)]\n",
    "print \"L3 (Logistic Loss) without ln(2): \", sum(logistic_losses)/len(logistic_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Logistic Regression on Training Set (Pre-2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  [u'Down' u'Up']\n",
      "coefficients:  [[-0.05547007 -0.04436492]]\n",
      "intercept : [ 0.0322169]\n"
     ]
    }
   ],
   "source": [
    "# dataset pre-2005--for training set\n",
    "x_train = df[(df['Year'] != 2005)][['Lag1', 'Lag2']]\n",
    "\n",
    "# directions pre-2005--for training set\n",
    "y_train = df[(df['Year'] != 2005)]['Direction']\n",
    "\n",
    "clf = skl_lm.LogisticRegression(solver='newton-cg')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "print 'classes: ',clf.classes_\n",
    "print 'coefficients: ',clf.coef_\n",
    "print 'intercept :', clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.055470069397048495, -0.044364924475245905]\n",
      "[0.032216904838310474]\n",
      "[0.032216904838310474, -0.055470069397048495, -0.044364924475245905]\n"
     ]
    }
   ],
   "source": [
    "# random code testing\n",
    "a = clf.coef_[0].tolist()\n",
    "b = clf.intercept_.tolist()\n",
    "\n",
    "print a\n",
    "print b\n",
    "print b+a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire Probabilities of Direction==Yes for our Test Set (2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data in 2005--for testing set\n",
    "x_test = df[(df['Year'] == 2005)][['Lag1', 'Lag2']]\n",
    "\n",
    "# to compare with predictions from test set later (the ACTUAL directions in 2005)\n",
    "y_test_unicode = df[(df['Year'] == 2005)]['Direction'].tolist()\n",
    "y_test = [x.encode('ascii') for x in y_test_unicode]\n",
    "\n",
    "# 252 X 2 Matrix, where column = probability for DOWN direction, probability for UP direction--reference: clf.classes_\n",
    "prob = clf.predict_proba(x_test)\n",
    "\n",
    "# predicted probabilities in 2005 for UP direction\n",
    "prob_up = prob[:,1:2]\n",
    "\n",
    "# convert to list\n",
    "prob_up2 = [i.tolist()[0] for i in prob_up]\n",
    "\n",
    "y_predicted = []\n",
    "for i in prob_up2:\n",
    "    if i>0.5:\n",
    "        y_predicted.append(\"Up\")\n",
    "    else:\n",
    "        y_predicted.append(\"Down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1: 0-1 Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 (0-1 loss):  0.440476190476  which matches the book!\n"
     ]
    }
   ],
   "source": [
    "right_prediction = [i for i,j in zip(y_test, y_predicted) if i==j]\n",
    "number_right = len(right_prediction)\n",
    "zero_one_loss = number_right/len(y_predicted)\n",
    "\n",
    "# I had to get the percentage wrong; not the percentage correct (same as error rate in classification)\n",
    "print \"L1 (0-1 loss): \", 1-zero_one_loss, \" which matches the book!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 (Cross Entropy Loss) by sci-kit:  0.689785443022\n",
      "L2 (Cross Entropy Loss) by manually:  0.689785443022\n"
     ]
    }
   ],
   "source": [
    "# source: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "#         https://en.wikipedia.org/wiki/Cross_entropy\n",
    "\n",
    "# using sci-kit:\n",
    "y_test2 = []\n",
    "for i in y_test:\n",
    "    if i=='Up':\n",
    "        y_test2.append(1)\n",
    "    else:\n",
    "        y_test2.append(0)\n",
    "\n",
    "print \"L2 (Cross Entropy Loss) by sci-kit: \", log_loss(y_test2, prob_up2)\n",
    "\n",
    "# manually:\n",
    "def ln(x):\n",
    "    return np.log(x)\n",
    "\n",
    "cross_entropy_losses = [-i*ln(j)-(1-i)*ln(1-j) for i,j in zip(y_test2, prob_up2)]\n",
    "print \"L2 (Cross Entropy Loss) by manually: \", sum(cross_entropy_losses)/len(cross_entropy_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3 Logistic Loss with ln(2) vs. without ln(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L3 (Logistic Loss) with ln(2):  0.820587590405\n",
      "L3 (Logistic Loss) without ln(2):  0.568787974691\n"
     ]
    }
   ],
   "source": [
    "# source (with ln(2)): https://en.wikipedia.org/wiki/Loss_functions_for_classification\n",
    "\n",
    "# source (without ln(2)): https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf\n",
    "#                         https://github.com/JohnLangford/vowpal_wabbit/wiki/Loss-functions\n",
    "#                         http://www.cs.cmu.edu/~yandongl/loss.html\n",
    "\n",
    "logistic_losses = [(1/ln(2))*(ln(1+np.exp(-i*j))) for i,j in zip(y_test2, prob_up2)]        \n",
    "print \"L3 (Logistic Loss) with ln(2): \", sum(logistic_losses)/len(logistic_losses)\n",
    "\n",
    "logistic_losses = [ln(1+np.exp(-i*j)) for i,j in zip(y_test2, prob_up2)]\n",
    "print \"L3 (Logistic Loss) without ln(2): \", sum(logistic_losses)/len(logistic_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                     Loss Functions  Train->Train (All)  Train->Test (2005)\n",
       "0                     L1 (0-1 Loss)            0.472000            0.440476\n",
       "1           L2 (Cross Entropy Loss)            0.691361            0.689785\n",
       "2  L3 (Logistic Loss) without ln(2)            0.575827            0.568788\n",
       "3     L3 (Logistic Loss) with ln(2)            0.830742            0.820588"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_functions = ['L1 (0-1 Loss)', 'L2 (Cross Entropy Loss)', 'L3 (Logistic Loss) without ln(2)', 'L3 (Logistic Loss) with ln(2)']\n",
    "train_train_scores = [0.472, 0.69136137137, 0.575826819281, 0.830742496587]\n",
    "train_test_scores = [0.440476190476, 0.689785443022, 0.568787974691, 0.820587590405]\n",
    "data = {'Loss Functions': loss_functions,\n",
    "        'Train->Train (All)': train_train_scores,\n",
    "        'Train->Test (2005)': train_test_scores\n",
    "       }\n",
    "DF = pd.DataFrame(data, columns = ['Loss Functions', 'Train->Train (All)', 'Train->Test (2005)'])\n",
    "\n",
    "DF"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
