{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to compare and contrast case clustering based on the opinion text (natural language processing) vs. based on the citation structure (network community detection). \n",
    "\n",
    "Commmunity detection on the network\n",
    "- modularity\n",
    "- walktrap\n",
    "- SBM (todo)\n",
    "\n",
    "Clustering on the opinion texts\n",
    "- compute TD-IDF vectors of opinions\n",
    "    - KNN on tdidf vectors\n",
    "    - Gaussian mixture models (TODO)\n",
    "    - spectral clustering on similarity matrix (TODO)\n",
    "- topic modeling (TODO)\n",
    "    - LDA\n",
    "    - nonegative matrix factorization\n",
    "    \n",
    "Relational topic models (see blei paper) (TODO)\n",
    "\n",
    "### TODO\n",
    "- match clusters from different algos\n",
    "- find representatives for clusters\n",
    "    - top td-idf words\n",
    "    - 'most central case' in community (is this a thing?)\n",
    "- more CD algos\n",
    "    - fix number of communities same as number of NLP clusters \n",
    "- more NLP based clustering algos\n",
    "\n",
    "# Notes\n",
    "\n",
    "borrowing some code from http://brandonrose.org/clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# modify these for your own computer\n",
    "#repo_directory = '/Users/iaincarmichael/Dropbox/Research/law/law-net/'\n",
    "repo_directory = '/Users/Michael/Documents/GitHub/law-net/'\n",
    "\n",
    "#data_dir = '/Users/iaincarmichael/Documents/courtlistener/data/'\n",
    "data_dir = '/Users/Michael/Desktop/network_data/'\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# graph package\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "# stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# our code\n",
    "sys.path.append(repo_directory + 'code/')\n",
    "\n",
    "\n",
    "sys.path.append(repo_directory + 'vertex_metrics_experiment/code/')\n",
    "from bag_of_words import load_tf_idf\n",
    "\n",
    "\n",
    "# which network to download data for\n",
    "network_name = 'scotus' # 'federal', 'ca1', etc\n",
    "\n",
    "\n",
    "# some sub directories that get used\n",
    "raw_dir = data_dir + 'raw/'\n",
    "subnet_dir = data_dir + network_name + '/'\n",
    "text_dir = subnet_dir + 'textfiles/'\n",
    "nlp_dir = subnet_dir + 'nlp/'\n",
    "\n",
    "\n",
    "# jupyter notebook settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the graph\n",
    "G = ig.Graph.Read_GraphML(subnet_dir + network_name +'_network.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# largest connected component\n",
    "\n",
    "restrict our attention to the largest connected componenet on the network. also we are missing some text files from 2016 so lets ignore 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# limit ourselves to cases upto and including 2015 since we are missing some textfiles from 2016\n",
    "G = G.subgraph(G.vs.select(year_le=2015))\n",
    "\n",
    "# make graph undirected\n",
    "Gud = G.copy()\n",
    "Gud = Gud.as_undirected()\n",
    "\n",
    "# get largest connected componenet\n",
    "components = Gud.clusters(mode='STRONG')\n",
    "g = components.subgraphs()[np.argmax(components.sizes())]\n",
    "\n",
    "# CL ids of cases in largest connected component\n",
    "CLids = g.vs['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph clustering\n",
    "\n",
    "Do community detection on network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# modularity clustering\n",
    "cd_modularity = g.community_fastgreedy() # .as_clustering().membership\n",
    "\n",
    "mod_clust = cd_modularity.as_clustering()\n",
    "\n",
    "mod_clust.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph_clusters = pd.Series(mod_clust.membership, index=g.vs['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## walktrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %time cd_walktrap = g.community_walktrap()\n",
    "\n",
    "# wt_clust = cd_walktrap.as_clustering()\n",
    "\n",
    "# wt_clust.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load td-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix, op_id_to_bow_id = load_tf_idf(nlp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K means clustering on td-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 58min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set number of clusters\n",
    "num_clusters = 30\n",
    "\n",
    "# run kmeans\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "nlp_clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Compare NLP vs graph clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_text_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-a4bffc1ebb69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalized_text_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nlp'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'graph'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# add in NLP clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nlp'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp_clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalized_text_dict' is not defined"
     ]
    }
   ],
   "source": [
    "clusters = pd.DataFrame(index=normalized_text_dict.keys(), columns=['nlp', 'graph'])\n",
    "\n",
    "# add in NLP clusters\n",
    "clusters['nlp'] = nlp_clusters\n",
    "\n",
    "\n",
    "# add in communities \n",
    "clusters['graph'] = graph_clusters\n",
    "\n",
    "# consider nodes not considered in CD to be their own cluster\n",
    "# i.e. nodes outside the largest connected component\n",
    "clusters['graph'].fillna(max(graph_clusters) + 1, inplace=True)\n",
    "\n",
    "# make formatting\n",
    "clusters['graph'] = clusters['graph'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-38e1bbdc75b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: match clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
