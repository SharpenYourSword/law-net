{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "repo_directory = '/Users/iaincarmichael/Dropbox/Research/law/law-net/'\n",
    "data_dir = '/Users/iaincarmichael/data/courtlistener/'\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# stat\n",
    "import numpy as np\n",
    "import cPickle as picklefrom \n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# our code\n",
    "sys.path.append(repo_directory + 'code/')\n",
    "\n",
    "sys.path.append(repo_directory + 'vertex_metrics_experiment/code/')\n",
    "\n",
    "from pipeline_helper_functions import save_sparse_csr, load_sparse_csr\n",
    "\n",
    "\n",
    "# which network to download data for\n",
    "network_name = 'scotus' # 'federal', 'ca1', etc\n",
    "\n",
    "# some sub directories that get used\n",
    "raw_dir = data_dir + 'raw/'\n",
    "subnet_dir = data_dir + network_name + '/'\n",
    "text_dir = subnet_dir + 'textfiles/'\n",
    "nlp_dir = subnet_dir + 'nlp/'\n",
    "\n",
    "\n",
    "# jupyter notebook settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = load_sparse_csr(nlp_dir + 'tfidf_matrix.npz')\n",
    "\n",
    "with open(nlp_dir + 'op_id_to_bow_id.p', 'rb') as f:\n",
    "    op_id_to_bow_id = pickle.load(f)\n",
    "\n",
    "with open(nlp_dir + 'vocab.p', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TF():\n",
    "    \"\"\"\n",
    "    Class that stores the tfidf matrix and...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nlp_dir):\n",
    "        \n",
    "        self.tfidf_matrix = load_sparse_csr(nlp_dir + 'tfidf_matrix.npz')\n",
    "\n",
    "        with open(nlp_dir + 'op_id_to_bow_id.p', 'rb') as f:\n",
    "            self.op_id_to_bow_id = pickle.load(f)\n",
    "\n",
    "        with open(nlp_dir + 'vocab.p', 'rb') as f:\n",
    "            self.vocab = np.array(pickle.load(f))\n",
    "\n",
    "\n",
    "    def top_k_words(self, opinions, num_words):\n",
    "        \"\"\"\n",
    "        This function summarizes a set of opinions by returning the words that appear in these opinions with the highest tf-idf scores.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        opinions: list of opinion ids\n",
    "        num_words: number of words to return as the summary\n",
    "        tfidf_matrix: the tf-idf matrix of all SCOTUS opinions\n",
    "        op_id_to_bow_id: dict that maps opinion ids to rows of the tfidf matrix\n",
    "\n",
    "        Output\n",
    "        -------\n",
    "        a list of the words with highest tf-idf scores amount the given opinions\n",
    "        \"\"\"\n",
    "\n",
    "        # op_id_to_bow_id['opinion_id'] = 'row_index'\n",
    "\n",
    "        n = num_words\n",
    "        row_indices = []\n",
    "\n",
    "        # get row indices corresponding to the opinions\n",
    "        for each_opinion in opinions:\n",
    "            row_index = self.op_id_to_bow_id[each_opinion]\n",
    "            row_indices.append(row_index)\n",
    "\n",
    "        # construct matrix with rows (opinions) from cluster\n",
    "        new_matrix = self.tfidf_matrix[row_indices, :]\n",
    "\n",
    "        # return the matrix as sorted listed-of-tuples (descending sort by tf-idf values)\n",
    "        sorted_matrix = sort_coo(new_matrix)\n",
    "\n",
    "        # get the unique column indices\n",
    "        column_ind = [x[1] for x in sorted_matrix]\n",
    "        column_ind = f7(column_ind) # unique and same ordering\n",
    "\n",
    "        # get the words from column indice        \n",
    "        top_words = self.vocab[column_ind].tolist()[:n]\n",
    "        return top_words\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blah = TF(nlp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'allot',\n",
       " u'bulk',\n",
       " u'tobacco',\n",
       " u'plant',\n",
       " u'agricultur',\n",
       " u'commod',\n",
       " u'skelton',\n",
       " u'deed',\n",
       " u'farm',\n",
       " u'land']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blah.top_k_words(['98286', '105366'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_top_n_clusters(n, total_number_clusters, graph_clusters):\n",
    "    \"\"\"\n",
    "    for modularity/walktrap:\n",
    "    ------------------------\n",
    "        prints summary of top 'n' clusters\n",
    "        returns dictionary of top n clusters\n",
    "            (key = cluster #, value = list of opinions)\n",
    "    \n",
    "    parameters\n",
    "    -----------\n",
    "        n = number of top clusters\n",
    "        total_number_clusters = total number of clusters from clustering algorithm\n",
    "        graph_clusters = pd.Series form of graph_clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    clusters_size =[]\n",
    "    for i in range(0,total_number_clusters+1):\n",
    "        cluster_i = graph_clusters[graph_clusters == i].index.tolist() # list of opinions in cluster i\n",
    "        clusters_size.append((i,len(cluster_i))) # (cluster #, size_of_cluster)\n",
    "\n",
    "    # descending sort by size of cluster\n",
    "    clusters_size = sorted(clusters_size, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # get top 'n' biggest clusters\n",
    "    biggest_clusters = []\n",
    "    for i in clusters_size:\n",
    "        biggest_clusters.append(i[0])\n",
    "    biggest_clusters = biggest_clusters[0:n]\n",
    "\n",
    "    # summarize top 'n' biggest clusters\n",
    "    for i in clusters_size[0:n]:\n",
    "        print \"cluster\", i[0], \":\", i[1], \"opinions\"\n",
    "\n",
    "    clusters_dict = OrderedDict()\n",
    "    for i in clusters_size[0:n]:\n",
    "        cluster_i = graph_clusters[graph_clusters == i[0]].index.tolist() # list of opinions in cluster i\n",
    "        clusters_dict[i[0]] = cluster_i\n",
    "\n",
    "    return clusters_dict, biggest_clusters\n",
    "\n",
    "def sort_coo(m): # helper function\n",
    "    '''\n",
    "    iterating through a csr (compressed sparse row) matrix:\n",
    "    (row_index, column_index) tf_idf_value\n",
    "    \n",
    "    return a list of tuples (row, column, value), sorted by tf-idf values in descending order\n",
    "    '''\n",
    "    m = m.tocoo()\n",
    "    list_of_tuples = []\n",
    "    for i,j,k in zip(m.row, m.col, m.data):\n",
    "        list_of_tuples.append((i,j,k)) # list of tuples\n",
    "    return sorted(list_of_tuples, key=lambda x: x[2], reverse=True) # sort by tfidf values (descending)\n",
    "\n",
    "def f7(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "def all_opinions(file_paths): # helper function\n",
    "    '''\n",
    "    Get list of all opinions/text files from the (.txt) file paths\n",
    "    '''\n",
    "    \n",
    "    all_opinions = []\n",
    "    for i in file_paths:\n",
    "        num = re.search(r'(\\d+)', i)\n",
    "        num = num.group()\n",
    "        all_opinions.append(num)\n",
    "    \n",
    "    # sort the list\n",
    "    all_opinions = map(int, all_opinions) # convert all elements of list into type(int)\n",
    "    all_opinions.sort()\n",
    "    \n",
    "    # convert list back to list of strings\n",
    "    all_opinions = map(str, all_opinions)\n",
    "    \n",
    "    return all_opinions\n",
    "\n",
    "\n",
    "\n",
    "####################### Summarize Cluster 1 #######################\n",
    "def top_k_words(opinions, num_words, tfidf_matrix, op_id_to_bow_id, vocab):\n",
    "    \"\"\"\n",
    "    This function summarizes a set of opinions by returning the words that appear in these opinions with the highest tf-idf scores.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    opinions: list of opinion ids\n",
    "    num_words: number of words to return as the summary\n",
    "    tfidf_matrix: the tf-idf matrix of all SCOTUS opinions\n",
    "    op_id_to_bow_id: dict that maps opinion ids to rows of the tfidf matrix\n",
    "\n",
    "    Output\n",
    "    -------\n",
    "    a list of the words with highest tf-idf scores amount the given opinions\n",
    "    \"\"\"\n",
    "    \n",
    "    # op_id_to_bow_id['opinion_id'] = 'row_index'\n",
    "    \n",
    "    vocab = np.array(vocab)\n",
    "    n = num_words\n",
    "    row_indices = []\n",
    "    \n",
    "    # get row indices corresponding to the opinions\n",
    "    for each_opinion in opinions:\n",
    "        row_index = op_id_to_bow_id[each_opinion]\n",
    "        row_indices.append(row_index)\n",
    "    \n",
    "    # construct matrix with rows (opinions) from cluster\n",
    "    new_matrix = tfidf_matrix[row_indices, :]\n",
    "    \n",
    "    # return the matrix as sorted listed-of-tuples (descending sort by tf-idf values)\n",
    "    sorted_matrix = sort_coo(new_matrix)\n",
    "    \n",
    "    # get the unique column indices\n",
    "    column_ind = [x[1] for x in sorted_matrix]\n",
    "    column_ind = f7(column_ind) # unique and same ordering\n",
    "    \n",
    "    # get the words from column indices\n",
    "    top_words = vocab[column_ind].tolist()[:n]\n",
    "    return top_words\n",
    "\n",
    "\n",
    "\n",
    "####################### Summarize Cluster 2 #######################\n",
    "def top_k_words_from_mean_vector(opinions, num_words, tfidf_matrix, op_id_to_bow_id, vocab):\n",
    "    '''\n",
    "    compute the mean tf-idf vector of the cluster, return the top K words from this mean vector\n",
    "    '''\n",
    "    \n",
    "    # op_id_to_bow_id['opinion_id'] = 'row_index'\n",
    "\n",
    "    vocab = np.array(vocab)\n",
    "    n = num_words\n",
    "    row_indices = []\n",
    "    \n",
    "    # get row indices corresponding to the opinions\n",
    "    for each_opinion in opinions:\n",
    "        row_index = op_id_to_bow_id[each_opinion]\n",
    "        row_indices.append(row_index)\n",
    "    \n",
    "    # construct a matrix with rows (opinions) from cluster\n",
    "    new_matrix = tfidf_matrix[row_indices, :]\n",
    "    \n",
    "    # to take the mean of each col (use axis=1 to take mean of each row)\n",
    "    mean_matrix = new_matrix.mean(axis=0) # 1 X 567570 row matrix \n",
    "    \n",
    "    # get the column indices\n",
    "    column_ind = np.argsort(mean_matrix, axis=1)[:, ::-1] # descending order\n",
    "    \n",
    "    # get the words from column indices\n",
    "    top_words = vocab[column_ind].tolist()[0][:n]\n",
    "    return top_words\n",
    "\n",
    "\n",
    "\n",
    "####################### Summarize Cluster 3 #######################\n",
    "def top_k_words_from_difference(opinions, all_opinions, num_words, tfidf_matrix, op_id_to_bow_id, vocab):\n",
    "    '''\n",
    "    compute the mean tf-idf vector of the cluster and also of the complement of the cluster, \n",
    "    take the difference mu_cluster - mu_complement, return the top K words in this difference    \n",
    "    '''\n",
    "    \n",
    "    # op_id_to_bow_id['opinion_id'] = 'row_index'\n",
    "    \n",
    "    vocab = np.array(vocab)\n",
    "    n = num_words\n",
    "    row_indices = []\n",
    "    \n",
    "    # get row indices corresponding to the opinions\n",
    "    for each_opinion in opinions:\n",
    "        row_index = op_id_to_bow_id[each_opinion]\n",
    "        row_indices.append(row_index)\n",
    "    \n",
    "    # construct a matrix with rowss (opinions) from cluster\n",
    "    cluster_matrix = tfidf_matrix[row_indices, :]\n",
    "\n",
    "    # to take the mean of each col (use axis=1 to take mean of each row)\n",
    "    mean_matrix = cluster_matrix.mean(axis=0) # 1 X 567570 row matrix\n",
    "    \n",
    "    \n",
    "    \n",
    "    # complement of cluster (all the other opinions)\n",
    "    opinions_compl = [x for x in all_opinions if x not in opinions]\n",
    "    \n",
    "    # get row indices corresponding to complement of cluster\n",
    "    row_indices_compl = []\n",
    "    for each_opinion in opinions_compl:\n",
    "        row_index = op_id_to_bow_id[each_opinion]\n",
    "        row_indices_compl.append(row_index)\n",
    "    \n",
    "    # construct a matrix with rows (opinions) from complement of cluster\n",
    "    compl_matrix = tfidf_matrix[row_indices_compl, :]\n",
    "    \n",
    "    # to take the mean of each col (use axis=1 to take mean of each row)\n",
    "    mean_matrix_compl = compl_matrix.mean(axis=0) # 1 X 567570 row matrix\n",
    "    \n",
    "    \n",
    "    \n",
    "    # mu_cluster - mu_complement\n",
    "    final_mean_matrix = mean_matrix - mean_matrix_compl\n",
    "    \n",
    "    # get the column indices\n",
    "    column_ind = np.argsort(final_mean_matrix, axis=1)[:, ::-1] # descending order\n",
    "    \n",
    "    # get the words from column indices\n",
    "    top_words = vocab[column_ind].tolist()[0][:n]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "\n",
    "\n",
    "####################### Summarize Cluster 4 #######################\n",
    "def document_closest_to_mean(opinions, tfidf_matrix, op_id_to_bow_id):\n",
    "    '''\n",
    "    compute the mean tf-idf vector, return the document in the cluster closet to the mean  \n",
    "    '''\n",
    "    \n",
    "    # op_id_to_bow_id['opinion_id'] = 'row_index'\n",
    "\n",
    "    row_indices = []\n",
    "    \n",
    "    # get row indices corresponding to the opinions\n",
    "    for each_opinion in opinions:\n",
    "        row_index = op_id_to_bow_id[each_opinion]\n",
    "        row_indices.append(row_index)\n",
    "    \n",
    "    # construct a matrix with rows (opinions) from cluster\n",
    "    new_matrix = tfidf_matrix[row_indices, :]\n",
    "    \n",
    "    # to take the mean of each col (use axis=1 to take mean of each row)\n",
    "    mean_matrix = new_matrix.mean(axis=0) # 1 X 567570 row matrix\n",
    "    \n",
    "    # convert to vector (since row matrix)\n",
    "    mean_vector = np.squeeze(np.asarray(mean_matrix))\n",
    "    \n",
    "    # get the euclidean distance between mean vector and all other cluster, row vectors\n",
    "    euc_dist = {}\n",
    "    for i in row_indices:\n",
    "        row_vector = np.squeeze(np.asarray(tfidf_matrix[i].toarray()))\n",
    "        euc_dist[i] = np.linalg.norm(mean_vector-row_vector)\n",
    "    \n",
    "    # get row index closest to mean vector (minimum euclidian distance to mean vector)\n",
    "    row_index_close = min(euc_dist, key=euc_dist.get)\n",
    "    \n",
    "    # get opinion closest to mean vector\n",
    "    for opinion, row_index in op_id_to_bow_id.iteritems():\n",
    "        if row_index == row_index_close:\n",
    "            return opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
