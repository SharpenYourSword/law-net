{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get similarity scores for a list of cases\n",
    "\n",
    "Alternative for get_similarities() from similiarity_matrix.py\n",
    "\n",
    "Uses closure for workhorse function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_get_similarity(similarity_matrix, CLid_to_index):\n",
    "    \"\"\"\n",
    "    helper funciton for get_similarities\n",
    "    \"\"\"\n",
    "    def f(CLid_pair):\n",
    "        try:\n",
    "            ida = CLid_to_index[CLid_pair[0]]\n",
    "            idb = CLid_to_index[CLid_pair[1]]\n",
    "        \n",
    "            return similarity_matrix[ida, idb]\n",
    "        except KeyError:\n",
    "            return np.nan\n",
    "        \n",
    "    return f\n",
    "    \n",
    "def get_similarities(similarity_matrix, CLid_pair, CLid_to_index):\n",
    "    \"\"\"\n",
    "    Returns the similarities for cases index by CL ids as a list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    similarity_matrix: precomputed similarity matrix\n",
    "\n",
    "    CLid_A, CLid_B: two lists of CL ids whose similarities we want\n",
    "\n",
    "    CLid_to_index: dict that maps CL ids to similarity_matrix indices\n",
    "    \"\"\"\n",
    "    get_similarity = make_get_similarity(similarity_matrix, CLid_to_index)\n",
    "    return [get_similarity( pair) for pair in CLid_pair]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample absent edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this ended up being a lot slower than sampling an arbitrary pair of vertices then checking the conditions\n",
    "def sample_absent_edges(G, num_samples, active_years, seed=None):\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "        \n",
    "    # citing cases must be in active_years\n",
    "    possible_ing_cases = set(G.vs.select(year_ge = min(active_years), year_le = max(active_years)))\n",
    "\n",
    "    samples = set()\n",
    "    present_edges = set(G.get_edgelist()) \n",
    "    \n",
    "    while len(samples) < num_samples:\n",
    "        # sample a citing case\n",
    "        ing_vertex = random.sample(possible_ing_cases, 1)[0]\n",
    "        \n",
    "        # get possible cited cases\n",
    "        # cited cases must be strictly before citing case\n",
    "        possible_ed_cases = G.vs.select(year_le = ing_vertex['year'] - 1)\n",
    "        \n",
    "        # sample a cited case\n",
    "        ed_vertex = random.sample(possible_ed_cases, 1)[0]\n",
    "        \n",
    "        absent_edge = (ing_vertex.index, ed_vertex.index)\n",
    "        \n",
    "        if (absent_edge not in present_edges) and (absent_edge not in samples):\n",
    "            samples.add(absent_edge)\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textfile chunk iterator\n",
    "\n",
    "this could be helpful\n",
    "http://stackoverflow.com/questions/31784011/scikit-learn-fitting-data-into-chunks-vs-fitting-it-all-at-once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class textfile_chunks:\n",
    "    def __init__(self, paths, chunk_size):\n",
    "        self.i = 0\n",
    "\n",
    "        self.paths = paths\n",
    "        self.chunk_size = chunk_size\n",
    "        self.num_files = len(paths)\n",
    "\n",
    "        self.num_chunks = ceil(float(self.num_files)/self.chunk_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        if self.i < self.num_chunks:\n",
    "\n",
    "            # file paths to return\n",
    "            file_paths = self.paths[self.i:min(self.i + self.chunk_size,\n",
    "                                    self.num_files)]\n",
    "\n",
    "            # read in files and put them into dict\n",
    "            files = {}\n",
    "            for path in file_paths:\n",
    "                text = open(path, 'r').read()\n",
    "                files[path] = text_normalization(text)\n",
    "\n",
    "            self.i += 1\n",
    "\n",
    "            return files\n",
    "\n",
    "        else:\n",
    "            raise StopIteration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time aware page rank\n",
    "\n",
    "Functions to compute time aware pagerank. \n",
    "- does not use sparse matrices\n",
    "- does not use discreteMarkovChain package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_time_aware_pagerank_matrix(A, years, p, qtv, qvt):\n",
    "    \"\"\"\n",
    "    Returns the transition matrix of the time aware PageRank random walk\n",
    "    defined as follows. This method does not take advantage of sparse matrices\n",
    "    for intermediate computation.\n",
    "\n",
    "    Create bipartide time-as-a-node graph F\n",
    "    - include time as a node i.e. vertices are V(G) U V(G).years\n",
    "    - F contains a copy of G\n",
    "    - edge from each vetex to AND from its year\n",
    "    - edges go from each year to the following year\n",
    "\n",
    "    When the random walk is at a vertex of G\n",
    "    - probability qvt transitions to the time node\n",
    "    - probability 1 - qvt does a PageRank move\n",
    "\n",
    "    When the random walk is at a time node\n",
    "    - probability qtv transitions to a vertex in G (of the corresponding year)\n",
    "    - probability 1 - qtv moves to the next year\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A: adjacency matrix of original matrix where Aij = 1\n",
    "    iff there is an edge from i to j\n",
    "\n",
    "    Y: the years assigned to each node\n",
    "\n",
    "    p: PageRank parameter\n",
    "\n",
    "    qtv: probability of transitioning from time to vertex in original graph\n",
    "\n",
    "    qvt: probability of transitioning from vertx to time\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    P: the transition matrix\n",
    "\n",
    "    \"\"\"\n",
    "    # number of vertices in the graph\n",
    "    n = A.shape[0]\n",
    "    outdegrees = A.sum(axis=1)\n",
    "\n",
    "    # zero index the years\n",
    "    Y = np.array(years) - min(years)\n",
    "    m = max(Y) + 1\n",
    "\n",
    "    # number of cases per year\n",
    "    cases_per_year = [0] * m\n",
    "    cases_per_year_counter = Counter(Y)\n",
    "    for k in cases_per_year_counter.keys():\n",
    "        cases_per_year[k] = cases_per_year_counter[k]\n",
    "\n",
    "    # PageRank transition matrix\n",
    "    # (see murphy 17.37)\n",
    "    D = np.diag([0 if d == 0 else 1.0/d for d in outdegrees])\n",
    "    z = [1.0/n if d == 0 else (1.0 - p) / n for d in outdegrees]\n",
    "    PR = p * np.dot(A.T, D) + np.outer([1] * n, z)\n",
    "\n",
    "    # Time-Time transition matrix\n",
    "    # ones below diagonal\n",
    "    TT = np.zeros((m, m))\n",
    "    TT[1:m, :m-1] = np.diag([1] * (m - 1))\n",
    "\n",
    "    # Vertex-Time transition matrix\n",
    "    # i-th column is the Y[i]th basis vector\n",
    "    VT = np.zeros((m, n))\n",
    "    identity_m = np.eye(m)  # for basis vectors\n",
    "    for i in range(n):\n",
    "        VT[:, i] = identity_m[:, Y[i]]\n",
    "\n",
    "    # Time-Vertex transition matrix\n",
    "    # VT transpose but entries are scaled by number of cases in the year\n",
    "    TV = np.zeros((n, m))\n",
    "    # 1 over number of cases per year\n",
    "    n_inv = [0 if cases_per_year[i] == 0 else 1.0/cases_per_year[i]\n",
    "             for i in range(m)]\n",
    "    for i in range(n):\n",
    "        TV[i, :] = identity_m[Y[i], :] * n_inv[Y[i]]\n",
    "\n",
    "    # normalization matrix for TV\n",
    "    qtv_diag = [0 if cases_per_year[i] == 0 else qtv for i in range(m)]\n",
    "    qtv_diag[-1] = 1  # last column of TT is zeros\n",
    "    Qtv = np.diag(qtv_diag)\n",
    "\n",
    "    # overall transition matrix\n",
    "    P = np.zeros((n + m, n + m))\n",
    "    P[:n, :n] = (1 - qvt) * PR  # upper left\n",
    "    P[n:, :-m] = qvt * VT  # lower left\n",
    "    P[:n, -m:] = np.dot(TV, Qtv)  # upper right\n",
    "    P[-m:, -m:] = np.dot(TT, np.eye(m) - Qtv)  # lower right\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "def get_time_aware_pagerank(A, years, p, qtv, qvt):\n",
    "    P = get_time_aware_pagerank_matrix(A, years, p, qtv, qvt)\n",
    "    # get PageRank values\n",
    "    leading_eig = get_leading_evector(P)\n",
    "    ta_pr = leading_eig[:n]\n",
    "    pr_years = leading_eig[-m:]\n",
    "\n",
    "    # normalize to probabilty vectors\n",
    "    return ta_pr/sum(ta_pr), pr_years/sum(pr_years)\n",
    "\n",
    "\n",
    "def get_leading_evector(M):\n",
    "    evals, evecs = np.linalg.eig(M)\n",
    "    # evals, evecs = sp.linalg.eig(M)\n",
    "\n",
    "    # there really has to be a more elegant way to do this\n",
    "    return np.real(evecs[:, np.argmax(evals)].reshape(-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking loss functions\n",
    "\n",
    "these take a sorted list instead of precomputed rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "def get_mean_rankscore(relevant, pi):\n",
    "    \"\"\"\n",
    "    Retuns the mean rank score for a ranking.\n",
    "    The rank score of a ranked case is defined to be\n",
    "\n",
    "    rank_score = 1 - rank/num_ancestors\n",
    "\n",
    "    the mean rank score is the mean of all the rank scores of the cited cases\n",
    "\n",
    "    Between 0 and 1. Smaller values are better. Random ranking is .5.\n",
    "\n",
    "    See Zanin et al. (pref attachemnt aging ...)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    R: list of cases that were cited\n",
    "\n",
    "    pi: ranking of ancestors\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    average of cited case ranks scores scores\n",
    "    \"\"\"\n",
    "\n",
    "    rank_scores = []\n",
    "\n",
    "    # number of ancestors\n",
    "    num_items = len(pi)\n",
    "\n",
    "    # compute rank score for each case test case actually cited\n",
    "    for r in relevant:\n",
    "\n",
    "        # where cited case was ranked ()\n",
    "        rank = np.where(pi == r)[0][0] + 1\n",
    "\n",
    "        # score the ranking\n",
    "        rank_score = float(rank) / num_items\n",
    "\n",
    "        rank_scores.append(rank_score)\n",
    "\n",
    "    return np.mean(rank_scores)\n",
    "\n",
    "\n",
    "def get_reciprocal_rank(relevant, pi):\n",
    "    \"\"\"\n",
    "    Returns the reciprocal rank 1 / r(c) where r(c)\n",
    "    is the rank of the best rank of the cited cases.\n",
    "\n",
    "    Between 0 and 1, smaller is better\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relevant: list of cases that were cited\n",
    "\n",
    "    pi: ranking of ancestors\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    reciprocal rank\n",
    "    \"\"\"\n",
    "\n",
    "    ranks = []\n",
    "\n",
    "    # compute rank score for each case test case actually cited\n",
    "    for r in relevant:\n",
    "\n",
    "        # where cited case was ranked ()\n",
    "        rank = np.where(pi == r)[0][0] + 1\n",
    "\n",
    "        ranks.append(rank)\n",
    "\n",
    "    return 1.0/min(ranks)\n",
    "\n",
    "\n",
    "def get_precision_at_K(relevant, pi, K):\n",
    "    \"\"\"\n",
    "    Returns the precision at K\n",
    "    P@K = num cited cases in top K poisitions of pi / K\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    relevant: list of cases that were cited\n",
    "\n",
    "    pi: ranking of ancestors\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    precision at K\n",
    "    \"\"\"\n",
    "\n",
    "    top_k = set(pi[:K])\n",
    "    precision_k = [1 for r in relevant if r in top_k]\n",
    "\n",
    "    return float(len(precision_k)) / K\n",
    "\n",
    "\n",
    "def get_error_rate(predictions):\n",
    "    \"\"\"\n",
    "    Returns the erro 0-1 classification prediction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: pd df with columns 'pred_prob' and 'y'\n",
    "    \"\"\"\n",
    "\n",
    "    cutoff = 0.5\n",
    "    # get list of predicted probs and citaiton indicators\n",
    "    y_act = predictions['y'].tolist()\n",
    "    y_pred = [1 if p > cutoff else 0 for p in predictions['pred_prob']]\n",
    "\n",
    "    return np.mean([1 if y_act[i] == y_pred[i] else 0\n",
    "                    for i in range(len(y_act))])\n",
    "\n",
    "\n",
    "def get_logloss(predictions):\n",
    "    \"\"\"\n",
    "    Returns the log-loss for a 0-1 classification predictions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    predictions: pd df with columns 'pred_prob' and 'y'\n",
    "    \"\"\"\n",
    "\n",
    "    # get list of predicted probs and citaiton indicators\n",
    "    y_act = predictions['y'].tolist()\n",
    "    prob = predictions['pred_prob'].tolist()\n",
    "\n",
    "    return logloss(y_act, prob)\n",
    "\n",
    "\n",
    "def logloss(act, pred):\n",
    "    \"\"\"\n",
    "    Returns the log loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    \"\"\"\n",
    "    epsilon = 1e-15\n",
    "    pred = sp.maximum(epsilon, pred)\n",
    "    pred = sp.minimum(1-epsilon, pred)\n",
    "    ll = sum(act*sp.log(pred) + sp.subtract(1, act)*sp.log(sp.subtract(1, pred)))\n",
    "    ll = ll * -1.0/len(act)\n",
    "    return ll\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
