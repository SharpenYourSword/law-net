{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Modularity Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# modify these for your own computer\n",
    "repo_directory = '/Users/Michael/Documents/GitHub/law-net/'\n",
    "\n",
    "data_dir = '/Users/Michael/Desktop/network_data/'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cPickle as pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "# graph package\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "# stat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import scipy.sparse\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "# our code\n",
    "sys.path.append(repo_directory + 'code/')\n",
    "from summarize_clusters import *\n",
    "\n",
    "sys.path.append(repo_directory + 'vertex_metrics_experiment/code/')\n",
    "from bag_of_words import * \n",
    "\n",
    "# which network to download data for\n",
    "network_name = 'scotus' # 'federal', 'ca1', etc\n",
    "\n",
    "\n",
    "# some sub directories that get used\n",
    "raw_dir = data_dir + 'raw/'\n",
    "subnet_dir = data_dir + network_name + '/'\n",
    "text_dir = subnet_dir + 'textfiles/'\n",
    "nlp_dir = subnet_dir + 'nlp/'\n",
    "nlp_sub_dir = nlp_dir + 'bow_tfidf/' #tfidf matrix (and other info, i.e. vocab) computed from bag-of-words matrix\n",
    "nlp_bow_dir = nlp_dir + 'bow/' #bag-of-words matrix (and other info, i.e. vocab)\n",
    "\n",
    "# all the file paths for .txt files\n",
    "file_paths = glob.glob(text_dir + '*.txt')\n",
    "\n",
    "# all opinions\n",
    "all_the_opinions = all_opinions(file_paths)\n",
    "\n",
    "\n",
    "# jupyter notebook settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load tf-idf vectors\n",
    "**tfidf_matrix** = (row_index, column_index): tf_idf value (**CSR FORMAT**)  \n",
    "**op_id_to_bow_id** = opinion_id (corresponds to row indices)  \n",
    "**vocab** = all the words in tfidf_matrix (correspond to column indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix, op_id_to_bow_id, vocab = load_tf_idf(nlp_sub_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Work:\n",
    "focus on largest connected component of **undirected scotus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the graph\n",
    "G = ig.Graph.Read_GraphML(subnet_dir + network_name +'_network.graphml')\n",
    "\n",
    "# limit ourselves to cases upto and including 2015 since we are missing some textfiles from 2016\n",
    "G = G.subgraph(G.vs.select(year_le=2015))\n",
    "\n",
    "# make graph undirected\n",
    "Gud = G.copy()\n",
    "Gud = Gud.as_undirected()\n",
    "\n",
    "# get largest connected componenet\n",
    "components = Gud.clusters(mode='STRONG')\n",
    "g = components.subgraphs()[np.argmax(components.sizes())]\n",
    "\n",
    "# CL ids of cases in largest connected component\n",
    "CLids = g.vs['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modularity on undirected scotus\n",
    "\"For a given division of the network's vertices into some modules, modularity reflects the concentration of edges within modules compared with random distribution of links between all nodes regardless of modules\"--*Wikipedia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering with 24724 elements and 126 clusters\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# modularity clustering\n",
    "cd_modularity = g.community_fastgreedy() # .as_clustering().membership\n",
    "\n",
    "mod_clust = cd_modularity.as_clustering()\n",
    "\n",
    "print mod_clust.summary()\n",
    "\n",
    "# save clusters in pandas\n",
    "graph_clusters = pd.Series(mod_clust.membership, index=g.vs['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the top 5 biggest clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes of top 5 biggest clusters:\n",
      "\n",
      "cluster 2 : 9273 opinions\n",
      "cluster 0 : 6870 opinions\n",
      "cluster 1 : 6234 opinions\n",
      "cluster 3 : 1458 opinions\n",
      "cluster 15 : 76 opinions\n"
     ]
    }
   ],
   "source": [
    "dict_top_n_clusters, biggest_n_clusters = get_top_n_clusters(5, len(mod_clust), graph_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Cluster Function 1: \"top_k_words\"\n",
    "This function summarizes a set of opinions by returning the words that appear in these opinions with the highest tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 words for...\n",
      "\n",
      "\u001b[1;31mcluster 2\u001b[0m: ['squier', 'reinsur', 'scophoni', 'reinsur', 'graeff', 'arbitr', 'arbitr', 'sugg', 'passport', 'arbitr', 'wenzel', 'vetterlein', 'meserv', 'tray', 'arbitr', 'softlit', 'metsker', 'artwar', 'vichi', 'huntley']\n",
      "\u001b[1;31mcluster 0\u001b[0m: ['dispensari', 'pension', 'fslic', 'wass', 'bicknel', 'milk', 'jumel', 'boom', 'merriam', 'ch', 'cheroke', 'cheroke', 'tea', 'vs', 'pilkey', 'cheroke', 'menard', 'airco', 'vs', 'aaf']\n",
      "\u001b[1;31mcluster 1\u001b[0m: ['baal', 'stumpf', 'lagrand', 'ree', 'lesag', 'bail', 'kaupp', 'ashcraft', 'penri', 'mazzei', 'juvenil', 'copyright', 'deport', 'partin', 'maloney', 'anastaplo', 'church', 'flag', 'fior', 'vaccin']\n",
      "\u001b[1;31mcluster 3\u001b[0m: ['shaeffer', 'wool', 'carusi', 'toy', 'seed', 'jen', 'paper', 'renfrow', 'pearl', 'cork', 'tile', 'cadet', 'pardon', 'postmast', 'hilsman', 'ore', 'collector', 'nail', 'hoppl', 'frerich']\n",
      "\u001b[1;31mcluster 15\u001b[0m: ['flaglor', 'dippold', 'nailor', 'goff', 'turpin', 'reeder', 'bilsland', 'shappirio', 'randel', 'forsyth', 'probat', 'godey', 'ellicott', 'orphan', 'neblett', 'mine', 'townsend', 'campbel', 'handi', 'lifter']\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "k=20 # number of words to get\n",
    "\n",
    "print \"top\", k, \"words for...\"\n",
    "print ''\n",
    "for i in biggest_n_clusters:\n",
    "    top_words = top_k_words(dict_top_n_clusters[i], k, tfidf_matrix, op_id_to_bow_id, vocab)\n",
    "    print '\\x1b[1;31m' + 'cluster ' + str(i) + '\\x1b[0m' + \":\", [x.encode('utf-8') for x in top_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Cluster Function 2: \"top_k_words_from_mean_vector\"\n",
    "compute the mean tf-idf vector of the cluster, return the top K words from this mean vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " top 20 words (from mean vector) for...\n",
      "\n",
      "\u001b[1;31mcluster 2\u001b[0m: ['court', 'state', 'v', 'case', 'compani', 'plaintiff', 'defend', 'act', 'upon', 'law', 'said', 'co', 'jurisdict', 'u', 'judgment', '\\xc2\\xa7', 'unit', 'decre', 'suit', 'contract']\n",
      "\u001b[1;31mcluster 0\u001b[0m: ['state', 'tax', 'court', 'land', 'v', 'act', 'compani', 'upon', 'case', 'unit', 'l', 'u', 'law', 'properti', 'ct', 'railroad', 'commiss', 'said', 'ed', 'co']\n",
      "\u001b[1;31mcluster 1\u001b[0m: ['court', 'state', 'v', 'sct', 'us', 'led2d', 'case', 'petition', '\\xc2\\xa7', 'unit', 'constitut', 'law', 'see', 'feder', 'trial', 'act', 'amend', 'would', 'juri', 'right']\n",
      "\u001b[1;31mcluster 3\u001b[0m: ['court', 'state', 'act', 'unit', 'case', 'v', 'upon', 'said', 'contract', 'offic', 'law', 'made', 'shall', 'defend', 'plaintiff', 'error', 'duti', 'u', 'section', 'claim']\n",
      "\u001b[1;31mcluster 15\u001b[0m: ['deed', 'court', 'properti', 'wife', 'convey', 'husband', 'estat', 'said', 'land', 'upon', 'case', 'v', 'titl', 'decre', 'bill', 'defend', 'made', 'plaintiff', 'claim', 'complain']\n",
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "k=20 # number of words to get\n",
    "\n",
    "print \"top\", k, \"words (from mean vector) for...\"\n",
    "print ''\n",
    "for i in biggest_n_clusters:\n",
    "    top_words_from_mean = top_k_words_from_mean_vector(dict_top_n_clusters[i], k, tfidf_matrix, op_id_to_bow_id, vocab)\n",
    "    print '\\x1b[1;31m' + 'cluster ' + str(i) + '\\x1b[0m' + \":\", [x.encode('utf-8') for x in top_words_from_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Cluster Function 3: \"top_k_words_from_difference\"\n",
    "compute the mean tf-idf vector of the cluster and also of the complement of the cluster,  \n",
    "take the difference mu_cluster - mu_complement, return the top K words in this difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 20 words (from mean difference) for...\n",
      "\n",
      "\u001b[1;31mcluster 2\u001b[0m: ['plaintiff', 'bankruptci', 'court', 'suit', 'patent', 'jurisdict', 'compani', 'creditor', 'decre', 'defend', 'bank', 'circuit', 'co', 'neglig', 'employe', 'action', 'bond', 'bankrupt', 'mortgag', 'parti']\n",
      "\u001b[1;31mcluster 0\u001b[0m: ['tax', 'land', 'state', 'compani', 'commiss', 'indian', 'railroad', 'rate', 'l', 'ct', 'incom', 'interst', 'commerc', 'ed', 'corpor', 'properti', 'act', 'carrier', 'taxat', 'water']\n",
      "\u001b[1;31mcluster 1\u001b[0m: ['led2d', 'sct', 'us', 'petition', 'v', 'convict', 'constitut', 'sentenc', 'state', 'crimin', 'see', 'amend', 'trial', 'feder', 'juri', '\\xc2\\xa7', 'search', 'habea', 'court', 'school']\n",
      "\u001b[1;31mcluster 3\u001b[0m: ['indict', 'offic', 'collector', 'duti', 'unit', 'contract', 'navi', 'claimant', 'treasuri', 'act', 'servic', 'depart', 'articl', 'cent', 'shall', 'apprais', 'govern', 'section', 'charg', 'made']\n",
      "\u001b[1;31mcluster 15\u001b[0m: ['deed', 'wife', 'convey', 'husband', 'estat', 'properti', 'titl', 'lot', 'said', 'complain', 'creditor', 'bill', 'purchas', 'decre', 'mortgag', 'orphan', 'execut', 'possess', 'premis', 'trust']\n",
      "Wall time: 26.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "k=20 # number of words to get\n",
    "\n",
    "print \"top\", k, \"words (from mean difference) for...\"\n",
    "print ''\n",
    "for i in biggest_n_clusters:\n",
    "    top_words_from_diff = top_k_words_from_difference(dict_top_n_clusters[i], all_the_opinions, \n",
    "                                                      k, tfidf_matrix, op_id_to_bow_id, vocab)\n",
    "    print '\\x1b[1;31m' + 'cluster ' + str(i) + '\\x1b[0m' + \":\", [x.encode('utf-8') for x in top_words_from_diff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Cluster Function 4: \"document_closest_to_mean\"\n",
    "compute the mean tf-idf vector, return the document in the cluster closet to the mean  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most relevant document for...\n",
      "\n",
      "\u001b[1;31mcluster 2\u001b[0m: opinion 89905\n",
      "\u001b[1;31mcluster 0\u001b[0m: opinion 95354\n",
      "\u001b[1;31mcluster 1\u001b[0m: opinion 104135\n",
      "\u001b[1;31mcluster 3\u001b[0m: opinion 86062\n",
      "\u001b[1;31mcluster 15\u001b[0m: opinion 87645\n",
      "Wall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print \"most relevant document for...\"\n",
    "print ''\n",
    "for i in biggest_n_clusters:\n",
    "    most_relev_op = document_closest_to_mean(dict_top_n_clusters[i], tfidf_matrix, op_id_to_bow_id)\n",
    "    print '\\x1b[1;31m' + 'cluster ' + str(i) + '\\x1b[0m' + \": opinion \" + most_relev_op"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
